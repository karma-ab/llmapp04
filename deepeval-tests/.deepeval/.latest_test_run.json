{"testRunData": {"testFile": "test_classify.py", "testCases": [{"name": "test_classify_schema_compliance[test_case0]", "input": "Python 3.12 introduces several new features including improved error messages, a new type parameter syntax, and performance improvements in the interpreter.", "actualOutput": "{\"labels\": [\"programming\", \"python\", \"software development\"], \"primaryCategory\": \"technology\", \"confidence\": 0.95}", "expectedOutput": "{\"labels\": [\"technology\", \"programming\", \"software\"], \"primaryCategory\": \"technology\", \"confidence\": 0.9}", "success": true, "metricsData": [{"name": "JSON Schema Compliance [GEval]", "threshold": 0.5, "success": true, "score": 1.0, "reason": "The actual output is valid JSON and parses without errors. It contains all required keys: 'labels', 'primaryCategory', and 'confidence'. 'labels' is an array of strings, 'primaryCategory' is a string, and 'confidence' is a number between 0 and 1. The structure, key names, and data types are all correct and complete.", "strictMode": false, "evaluationModel": "gpt-4.1", "evaluationCost": 0.002578, "verboseLogs": "Criteria:\nEvaluate whether the actual output is valid JSON that conforms to the required schema. Only check structure, key names, and data types \u2014 do NOT penalize for specific values. The JSON must contain: 'labels' (array of strings), 'primaryCategory' (string), and 'confidence' (number between 0 and 1). \n \nEvaluation Steps:\n[\n    \"Check if the actual output is valid JSON and can be parsed without errors.\",\n    \"Verify that the JSON contains the keys: 'labels', 'primaryCategory', and 'confidence'.\",\n    \"Ensure 'labels' is an array of strings, 'primaryCategory' is a string, and 'confidence' is a number between 0 and 1.\",\n    \"Compare outputs by how completely and accurately they meet the required structure, key names, and data types.\"\n] \n \nRubric:\nNone \n \nScore: 1.0"}], "runDuration": 6.083923541009426, "evaluationCost": 0.002578, "order": 0}, {"name": "test_classify_correctness[test_case0]", "input": "Python 3.12 introduces several new features including improved error messages, a new type parameter syntax, and performance improvements in the interpreter.", "actualOutput": "{\"labels\": [\"programming\", \"python\", \"software development\"], \"primaryCategory\": \"technology\", \"confidence\": 0.95}", "expectedOutput": "{\"labels\": [\"technology\", \"programming\", \"software\"], \"primaryCategory\": \"technology\", \"confidence\": 0.9}", "success": true, "metricsData": [{"name": "Classification Correctness [GEval]", "threshold": 0.5, "success": true, "score": 0.9, "reason": "The actual output's labels are highly relevant to the input text about Python 3.12 and closely match the expected output, with only minor differences in label wording (e.g., 'python' vs. 'software'). The primaryCategory 'technology' accurately represents the dominant topic and matches the expected output. The confidence score is a valid numeric value between 0 and 1. The only minor shortcoming is the slight variation in label specificity.", "strictMode": false, "evaluationModel": "gpt-4.1", "evaluationCost": 0.0029159999999999998, "verboseLogs": "Criteria:\nEvaluate whether the classification labels and primary category in the actual output are reasonable and accurate for the given input text. The labels should be topically relevant to the text content. The primaryCategory should represent the dominant topic. Closely related categories (e.g. 'business' vs 'finance') should be considered acceptable. The confidence score should be between 0 and 1. \n \nEvaluation Steps:\n[\n    \"Read the input text and identify its main topics and themes.\",\n    \"Check if the classification labels in the actual output are topically relevant to the input text and match or closely relate to the expected output labels.\",\n    \"Verify that the primaryCategory in the actual output accurately represents the dominant topic of the input text and is consistent with the expected output, allowing for closely related categories.\",\n    \"Ensure the confidence score in the actual output is a numeric value between 0 and 1.\"\n] \n \nRubric:\nNone \n \nScore: 0.9"}], "runDuration": 8.768042041978333, "evaluationCost": 0.0029159999999999998, "order": 0}, {"name": "test_classify_relevancy[test_case0]", "input": "Python 3.12 introduces several new features including improved error messages, a new type parameter syntax, and performance improvements in the interpreter.", "actualOutput": "{\"labels\": [\"programming\", \"python\", \"software development\"], \"primaryCategory\": \"technology\", \"confidence\": 0.95}", "expectedOutput": "{\"labels\": [\"technology\", \"programming\", \"software\"], \"primaryCategory\": \"technology\", \"confidence\": 0.9}", "success": true, "metricsData": [{"name": "Answer Relevancy [GEval]", "threshold": 0.5, "success": true, "score": 0.9029312229479407, "reason": "The output provides labels and a primary category that are directly relevant to the input text, which discusses new features in Python 3.12. The labels 'programming', 'python', and 'software development' accurately describe the subject matter, and 'technology' is an appropriate primary category. The confidence score is reasonable. The only minor shortcoming is that the output could be slightly more specific by referencing 'Python 3.12' or 'programming languages', but overall, it maintains strong topical relevance.", "strictMode": false, "evaluationModel": "gpt-4.1", "evaluationCost": 0.0028, "verboseLogs": "Criteria:\nEvaluate whether the actual output is topically relevant to the input text. The labels, categories, or analysis in the output should directly relate to the subject matter of the input. Structured metadata (labels, categories, confidence scores) that accurately describes the input text should be considered relevant. \n \nEvaluation Steps:\n[\n    \"Read the input text and identify its main topic or subject matter.\",\n    \"Examine the actual output to determine if its labels, categories, or analysis directly relate to the identified topic of the input text.\",\n    \"Check if any structured metadata in the output (such as labels, categories, or confidence scores) accurately describes the content of the input text.\",\n    \"Decide if the actual output maintains topical relevance to the input text throughout.\"\n] \n \nRubric:\nNone \n \nScore: 0.9029312229479407"}], "runDuration": 4.572370709036477, "evaluationCost": 0.0028, "order": 0}, {"name": "test_classify_schema_compliance[test_case1]", "input": "The Lakers defeated the Celtics 112-108 in overtime last night. LeBron James scored 35 points and had 10 assists in the victory.", "actualOutput": "{\"labels\": [\"sports\", \"basketball\", \"game\", \"result\", \"player\"], \"primaryCategory\": \"sports\", \"confidence\": 0.95}", "expectedOutput": "{\"labels\": [\"sports\", \"basketball\", \"NBA\"], \"primaryCategory\": \"sports\", \"confidence\": 0.9}", "success": true, "metricsData": [{"name": "JSON Schema Compliance [GEval]", "threshold": 0.5, "success": true, "score": 1.0, "reason": "The actual output is valid JSON and parses without errors. It contains all required keys: 'labels', 'primaryCategory', and 'confidence'. 'labels' is an array of strings, 'primaryCategory' is a string, and 'confidence' is a number between 0 and 1. The structure, key names, and data types are all correct and complete.", "strictMode": false, "evaluationModel": "gpt-4.1", "evaluationCost": 0.002598, "verboseLogs": "Criteria:\nEvaluate whether the actual output is valid JSON that conforms to the required schema. Only check structure, key names, and data types \u2014 do NOT penalize for specific values. The JSON must contain: 'labels' (array of strings), 'primaryCategory' (string), and 'confidence' (number between 0 and 1). \n \nEvaluation Steps:\n[\n    \"Check if the actual output is valid JSON and can be parsed without errors.\",\n    \"Verify that the JSON contains the keys: 'labels', 'primaryCategory', and 'confidence'.\",\n    \"Ensure that 'labels' is an array of strings, 'primaryCategory' is a string, and 'confidence' is a number between 0 and 1.\",\n    \"Compare outputs by how completely and accurately they meet the required structure, key names, and data types.\"\n] \n \nRubric:\nNone \n \nScore: 1.0"}], "runDuration": 5.769176083034836, "evaluationCost": 0.002598, "order": 1}, {"name": "test_classify_correctness[test_case1]", "input": "The Lakers defeated the Celtics 112-108 in overtime last night. LeBron James scored 35 points and had 10 assists in the victory.", "actualOutput": "{\"labels\": [\"sports\", \"basketball\", \"game\", \"result\", \"player\"], \"primaryCategory\": \"sports\", \"confidence\": 0.95}", "expectedOutput": "{\"labels\": [\"sports\", \"basketball\", \"NBA\"], \"primaryCategory\": \"sports\", \"confidence\": 0.9}", "success": true, "metricsData": [{"name": "Classification Correctness [GEval]", "threshold": 0.5, "success": true, "score": 0.9, "reason": "The main topics of the input are a basketball game result, specifically an NBA game involving the Lakers and Celtics, with a focus on LeBron James' performance. The actual output labels are all topically relevant, including 'sports' and 'basketball', and add 'game', 'result', and 'player', which are reasonable extensions. The primaryCategory 'sports' accurately represents the dominant topic and matches the expected output. The confidence score is a valid numeric value between 0 and 1. The only minor shortcoming is the omission of 'NBA' as a label, which is present in the expected output.", "strictMode": false, "evaluationModel": "gpt-4.1", "evaluationCost": 0.003222, "verboseLogs": "Criteria:\nEvaluate whether the classification labels and primary category in the actual output are reasonable and accurate for the given input text. The labels should be topically relevant to the text content. The primaryCategory should represent the dominant topic. Closely related categories (e.g. 'business' vs 'finance') should be considered acceptable. The confidence score should be between 0 and 1. \n \nEvaluation Steps:\n[\n    \"Read the input text and identify its main topics and themes.\",\n    \"Check if the classification labels in the actual output are topically relevant to the input text and match or closely align with the expected output labels.\",\n    \"Verify that the primaryCategory in the actual output accurately represents the dominant topic of the input text and is reasonable compared to the expected output, allowing for closely related categories.\",\n    \"Ensure the confidence score in the actual output is a numeric value between 0 and 1.\"\n] \n \nRubric:\nNone \n \nScore: 0.9"}], "runDuration": 6.122717750025913, "evaluationCost": 0.003222, "order": 1}, {"name": "test_classify_relevancy[test_case1]", "input": "The Lakers defeated the Celtics 112-108 in overtime last night. LeBron James scored 35 points and had 10 assists in the victory.", "actualOutput": "{\"labels\": [\"sports\", \"basketball\", \"game\", \"result\", \"player\"], \"primaryCategory\": \"sports\", \"confidence\": 0.95}", "expectedOutput": "{\"labels\": [\"sports\", \"basketball\", \"NBA\"], \"primaryCategory\": \"sports\", \"confidence\": 0.9}", "success": true, "metricsData": [{"name": "Answer Relevancy [GEval]", "threshold": 0.5, "success": true, "score": 1.0, "reason": "The output provides labels such as 'sports', 'basketball', 'game', 'result', and 'player', all of which are directly relevant to the input text about a basketball game result and player performance. The primary category 'sports' and a high confidence score accurately reflect the content. All evaluation steps are fully met.", "strictMode": false, "evaluationModel": "gpt-4.1", "evaluationCost": 0.0025399999999999997, "verboseLogs": "Criteria:\nEvaluate whether the actual output is topically relevant to the input text. The labels, categories, or analysis in the output should directly relate to the subject matter of the input. Structured metadata (labels, categories, confidence scores) that accurately describes the input text should be considered relevant. \n \nEvaluation Steps:\n[\n    \"Read the input text and identify its main topic or subject matter.\",\n    \"Examine the actual output to determine if its labels, categories, or analysis directly relate to the identified topic of the input.\",\n    \"Check if any structured metadata in the output (such as labels, categories, or confidence scores) accurately describes the content of the input text.\",\n    \"Decide if the actual output maintains topical relevance to the input based on the above checks.\"\n] \n \nRubric:\nNone \n \nScore: 1.0"}], "runDuration": 4.213032750005368, "evaluationCost": 0.0025399999999999997, "order": 1}, {"name": "test_classify_schema_compliance[test_case2]", "input": "To make a classic Italian carbonara, you need guanciale, eggs, pecorino romano cheese, black pepper, and spaghetti. Cook the pasta al dente and toss with the egg and cheese mixture.", "actualOutput": "{\"labels\": [\"recipe\", \"italian cuisine\", \"pasta\"], \"primaryCategory\": \"food\", \"confidence\": 0.95}", "expectedOutput": "{\"labels\": [\"food\", \"cooking\", \"recipe\"], \"primaryCategory\": \"food\", \"confidence\": 0.9}", "success": true, "metricsData": [{"name": "JSON Schema Compliance [GEval]", "threshold": 0.5, "success": true, "score": 1.0, "reason": "The actual output is valid JSON and parses without errors. It contains the required keys: 'labels', 'primaryCategory', and 'confidence'. 'labels' is an array of strings, 'primaryCategory' is a string, and 'confidence' is a number between 0 and 1. All evaluation steps are fully met.", "strictMode": false, "evaluationModel": "gpt-4.1", "evaluationCost": 0.002574, "verboseLogs": "Criteria:\nEvaluate whether the actual output is valid JSON that conforms to the required schema. Only check structure, key names, and data types \u2014 do NOT penalize for specific values. The JSON must contain: 'labels' (array of strings), 'primaryCategory' (string), and 'confidence' (number between 0 and 1). \n \nEvaluation Steps:\n[\n    \"Check if the actual output is valid JSON and can be parsed without errors.\",\n    \"Verify that the JSON contains the keys: 'labels', 'primaryCategory', and 'confidence'.\",\n    \"Ensure that 'labels' is an array of strings, 'primaryCategory' is a string, and 'confidence' is a number between 0 and 1 (inclusive).\",\n    \"Do not consider or penalize based on the specific values of the fields, only their structure and data types.\"\n] \n \nRubric:\nNone \n \nScore: 1.0"}], "runDuration": 12.313780499971472, "evaluationCost": 0.002574, "order": 2}, {"name": "test_classify_correctness[test_case2]", "input": "To make a classic Italian carbonara, you need guanciale, eggs, pecorino romano cheese, black pepper, and spaghetti. Cook the pasta al dente and toss with the egg and cheese mixture.", "actualOutput": "{\"labels\": [\"recipe\", \"italian cuisine\", \"pasta\"], \"primaryCategory\": \"food\", \"confidence\": 0.95}", "expectedOutput": "{\"labels\": [\"food\", \"cooking\", \"recipe\"], \"primaryCategory\": \"food\", \"confidence\": 0.9}", "success": true, "metricsData": [{"name": "Classification Correctness [GEval]", "threshold": 0.5, "success": true, "score": 0.9, "reason": "The actual output's labels ('recipe', 'italian cuisine', 'pasta') are all topically relevant to the input text about making Italian carbonara, though they differ slightly from the expected labels. The primaryCategory 'food' correctly matches the dominant topic of the input. The confidence score is a valid number between 0 and 1. The only minor shortcoming is that the labels do not exactly match the expected ones, but they remain highly relevant.", "strictMode": false, "evaluationModel": "gpt-4.1", "evaluationCost": 0.0028120000000000003, "verboseLogs": "Criteria:\nEvaluate whether the classification labels and primary category in the actual output are reasonable and accurate for the given input text. The labels should be topically relevant to the text content. The primaryCategory should represent the dominant topic. Closely related categories (e.g. 'business' vs 'finance') should be considered acceptable. The confidence score should be between 0 and 1. \n \nEvaluation Steps:\n[\n    \"Read the input text and identify its main topics and themes.\",\n    \"Check if the classification labels in the actual output are topically relevant to the input text.\",\n    \"Verify that the primaryCategory in the actual output matches the dominant topic of the input text, allowing for closely related categories as acceptable.\",\n    \"Ensure the confidence score in the actual output is a number between 0 and 1.\"\n] \n \nRubric:\nNone \n \nScore: 0.9"}], "runDuration": 6.166025375016034, "evaluationCost": 0.0028120000000000003, "order": 2}, {"name": "test_classify_relevancy[test_case2]", "input": "To make a classic Italian carbonara, you need guanciale, eggs, pecorino romano cheese, black pepper, and spaghetti. Cook the pasta al dente and toss with the egg and cheese mixture.", "actualOutput": "{\"labels\": [\"recipe\", \"italian cuisine\", \"pasta\"], \"primaryCategory\": \"food\", \"confidence\": 0.95}", "expectedOutput": "{\"labels\": [\"food\", \"cooking\", \"recipe\"], \"primaryCategory\": \"food\", \"confidence\": 0.9}", "success": true, "metricsData": [{"name": "Answer Relevancy [GEval]", "threshold": 0.5, "success": true, "score": 1.0, "reason": "The output provides labels such as 'recipe', 'italian cuisine', and 'pasta', all of which are directly relevant to the input text about making Italian carbonara. The primary category 'food' accurately describes the subject matter, and the high confidence score is appropriate. All structured metadata aligns well with the input, maintaining strong topical relevance.", "strictMode": false, "evaluationModel": "gpt-4.1", "evaluationCost": 0.00253, "verboseLogs": "Criteria:\nEvaluate whether the actual output is topically relevant to the input text. The labels, categories, or analysis in the output should directly relate to the subject matter of the input. Structured metadata (labels, categories, confidence scores) that accurately describes the input text should be considered relevant. \n \nEvaluation Steps:\n[\n    \"Read the input text and identify its main topic or subject matter.\",\n    \"Examine the actual output to determine if its labels, categories, or analysis directly relate to the input's topic.\",\n    \"Check if any structured metadata in the output (such as labels, categories, or confidence scores) accurately describes the input text.\",\n    \"Decide if the output maintains topical relevance to the input based on the above checks.\"\n] \n \nRubric:\nNone \n \nScore: 1.0"}], "runDuration": 5.443372207984794, "evaluationCost": 0.00253, "order": 2}, {"name": "test_classify_schema_compliance[test_case3]", "input": "The Federal Reserve announced it will maintain interest rates at their current level, citing concerns about inflation and the labor market outlook for the coming quarter.", "actualOutput": "{\"labels\": [\"economy\", \"finance\", \"interest rates\", \"inflation\", \"labor market\"], \"primaryCategory\": \"economics\", \"confidence\": 0.95}", "expectedOutput": "{\"labels\": [\"finance\", \"economics\", \"policy\"], \"primaryCategory\": \"finance\", \"confidence\": 0.9}", "success": true, "metricsData": [{"name": "JSON Schema Compliance [GEval]", "threshold": 0.5, "success": true, "score": 1.0, "reason": "The actual output is valid JSON and parses without errors. It contains all required keys: 'labels', 'primaryCategory', and 'confidence'. 'labels' is an array of strings, 'primaryCategory' is a string, and 'confidence' is a number between 0 and 1. The structure, key names, and data types are all correct and complete.", "strictMode": false, "evaluationModel": "gpt-4.1", "evaluationCost": 0.002606, "verboseLogs": "Criteria:\nEvaluate whether the actual output is valid JSON that conforms to the required schema. Only check structure, key names, and data types \u2014 do NOT penalize for specific values. The JSON must contain: 'labels' (array of strings), 'primaryCategory' (string), and 'confidence' (number between 0 and 1). \n \nEvaluation Steps:\n[\n    \"Check if the actual output is valid JSON and can be parsed without errors.\",\n    \"Verify that the JSON contains the keys: 'labels', 'primaryCategory', and 'confidence'.\",\n    \"Ensure 'labels' is an array of strings, 'primaryCategory' is a string, and 'confidence' is a number between 0 and 1.\",\n    \"Compare actual outputs by how completely and accurately they meet the required structure, key names, and data types.\"\n] \n \nRubric:\nNone \n \nScore: 1.0"}], "runDuration": 4.744628333020955, "evaluationCost": 0.002606, "order": 3}, {"name": "test_classify_correctness[test_case3]", "input": "The Federal Reserve announced it will maintain interest rates at their current level, citing concerns about inflation and the labor market outlook for the coming quarter.", "actualOutput": "{\"labels\": [\"economy\", \"finance\", \"interest rates\", \"inflation\", \"labor market\"], \"primaryCategory\": \"economics\", \"confidence\": 0.95}", "expectedOutput": "{\"labels\": [\"finance\", \"economics\", \"policy\"], \"primaryCategory\": \"finance\", \"confidence\": 0.9}", "success": true, "metricsData": [{"name": "Classification Correctness [GEval]", "threshold": 0.5, "success": true, "score": 0.8320821307318385, "reason": "The actual output provides labels that are highly relevant to the input text, including 'finance', 'interest rates', 'inflation', and 'labor market', which are all directly mentioned or implied. The primaryCategory 'economics' is closely related to the expected 'finance' and is reasonable given the content. The confidence score is a valid numeric value between 0 and 1. The only minor shortcoming is that the label set does not include 'policy', which is present in the expected output, and the primaryCategory is not an exact match but is still topically appropriate.", "strictMode": false, "evaluationModel": "gpt-4.1", "evaluationCost": 0.00317, "verboseLogs": "Criteria:\nEvaluate whether the classification labels and primary category in the actual output are reasonable and accurate for the given input text. The labels should be topically relevant to the text content. The primaryCategory should represent the dominant topic. Closely related categories (e.g. 'business' vs 'finance') should be considered acceptable. The confidence score should be between 0 and 1. \n \nEvaluation Steps:\n[\n    \"Read the input text and identify its main topics and themes.\",\n    \"Check if the classification labels in the actual output are topically relevant to the input text and match or closely align with the expected output labels.\",\n    \"Verify that the primaryCategory in the actual output accurately represents the dominant topic of the input text and is reasonable compared to the expected output, allowing for closely related categories.\",\n    \"Ensure the confidence score in the actual output is a numeric value between 0 and 1.\"\n] \n \nRubric:\nNone \n \nScore: 0.8320821307318385"}], "runDuration": 6.585220374981873, "evaluationCost": 0.00317, "order": 3}, {"name": "test_classify_relevancy[test_case3]", "input": "The Federal Reserve announced it will maintain interest rates at their current level, citing concerns about inflation and the labor market outlook for the coming quarter.", "actualOutput": "{\"labels\": [\"economy\", \"finance\", \"interest rates\", \"inflation\", \"labor market\"], \"primaryCategory\": \"economics\", \"confidence\": 0.95}", "expectedOutput": "{\"labels\": [\"finance\", \"economics\", \"policy\"], \"primaryCategory\": \"finance\", \"confidence\": 0.9}", "success": true, "metricsData": [{"name": "Answer Relevancy [GEval]", "threshold": 0.5, "success": true, "score": 1.0, "reason": "The output provides labels and a primary category that are directly relevant to the input, which discusses the Federal Reserve's decision on interest rates due to inflation and labor market concerns. The structured metadata accurately reflects the main topics, and the high confidence score is appropriate given the clear topical match.", "strictMode": false, "evaluationModel": "gpt-4.1", "evaluationCost": 0.002412, "verboseLogs": "Criteria:\nEvaluate whether the actual output is topically relevant to the input text. The labels, categories, or analysis in the output should directly relate to the subject matter of the input. Structured metadata (labels, categories, confidence scores) that accurately describes the input text should be considered relevant. \n \nEvaluation Steps:\n[\n    \"Read the input text and identify its main topic or subject matter.\",\n    \"Examine the actual output to determine if its labels, categories, or analysis directly relate to the input's topic.\",\n    \"Check if any structured metadata in the output (such as labels, categories, or confidence scores) accurately describes the input text.\",\n    \"Decide if the output maintains topical relevance to the input based on the above checks.\"\n] \n \nRubric:\nNone \n \nScore: 1.0"}], "runDuration": 8.906860125018284, "evaluationCost": 0.002412, "order": 3}, {"name": "test_classify_schema_compliance[test_case4]", "input": "A new study published in Nature shows that regular exercise can reduce the risk of heart disease by up to 30 percent and improve overall mental health outcomes.", "actualOutput": "{\"labels\": [\"health\", \"exercise\", \"research\", \"disease prevention\"], \"primaryCategory\": \"health\", \"confidence\": 0.95}", "expectedOutput": "{\"labels\": [\"health\", \"science\", \"medical\"], \"primaryCategory\": \"health\", \"confidence\": 0.9}", "success": true, "metricsData": [{"name": "JSON Schema Compliance [GEval]", "threshold": 0.5, "success": true, "score": 1.0, "reason": "The actual output is valid JSON and parses without errors. It contains all required keys: 'labels', 'primaryCategory', and 'confidence'. 'labels' is an array of strings, 'primaryCategory' is a string, and 'confidence' is a number between 0 and 1. The structure, key names, and data types are all correct and complete.", "strictMode": false, "evaluationModel": "gpt-4.1", "evaluationCost": 0.002584, "verboseLogs": "Criteria:\nEvaluate whether the actual output is valid JSON that conforms to the required schema. Only check structure, key names, and data types \u2014 do NOT penalize for specific values. The JSON must contain: 'labels' (array of strings), 'primaryCategory' (string), and 'confidence' (number between 0 and 1). \n \nEvaluation Steps:\n[\n    \"Check if the actual output is valid JSON and can be parsed without errors.\",\n    \"Verify that the JSON contains the keys: 'labels', 'primaryCategory', and 'confidence'.\",\n    \"Ensure 'labels' is an array of strings, 'primaryCategory' is a string, and 'confidence' is a number between 0 and 1.\",\n    \"Compare outputs by how completely and accurately they meet the required structure, key names, and data types.\"\n] \n \nRubric:\nNone \n \nScore: 1.0"}], "runDuration": 5.11983058397891, "evaluationCost": 0.002584, "order": 4}, {"name": "test_classify_correctness[test_case4]", "input": "A new study published in Nature shows that regular exercise can reduce the risk of heart disease by up to 30 percent and improve overall mental health outcomes.", "actualOutput": "{\"labels\": [\"health\", \"exercise\", \"research\", \"disease prevention\"], \"primaryCategory\": \"health\", \"confidence\": 0.95}", "expectedOutput": "{\"labels\": [\"health\", \"science\", \"medical\"], \"primaryCategory\": \"health\", \"confidence\": 0.9}", "success": true, "metricsData": [{"name": "Classification Correctness [GEval]", "threshold": 0.5, "success": true, "score": 0.9, "reason": "The main topics of the input are health, exercise, and disease prevention, which are well reflected in the actual output labels such as 'health', 'exercise', and 'disease prevention'. The label 'research' is also relevant given the mention of a study. The primaryCategory 'health' accurately represents the dominant topic. The confidence score is a valid numeric value between 0 and 1. The only minor shortcoming is the absence of the broader 'science' or 'medical' labels from the expected output, but the actual labels are still topically appropriate and specific.", "strictMode": false, "evaluationModel": "gpt-4.1", "evaluationCost": 0.003112, "verboseLogs": "Criteria:\nEvaluate whether the classification labels and primary category in the actual output are reasonable and accurate for the given input text. The labels should be topically relevant to the text content. The primaryCategory should represent the dominant topic. Closely related categories (e.g. 'business' vs 'finance') should be considered acceptable. The confidence score should be between 0 and 1. \n \nEvaluation Steps:\n[\n    \"Read the input text and identify its main topics and themes.\",\n    \"Check if the classification labels in the actual output are topically relevant to the input text and compare them to the expected output labels for reasonableness.\",\n    \"Verify that the primaryCategory in the actual output accurately represents the dominant topic of the input text, allowing for closely related categories as acceptable.\",\n    \"Ensure the confidence score in the actual output is a numeric value between 0 and 1.\"\n] \n \nRubric:\nNone \n \nScore: 0.9"}], "runDuration": 7.22924175002845, "evaluationCost": 0.003112, "order": 4}, {"name": "test_classify_relevancy[test_case4]", "input": "A new study published in Nature shows that regular exercise can reduce the risk of heart disease by up to 30 percent and improve overall mental health outcomes.", "actualOutput": "{\"labels\": [\"health\", \"exercise\", \"research\", \"disease prevention\"], \"primaryCategory\": \"health\", \"confidence\": 0.95}", "expectedOutput": "{\"labels\": [\"health\", \"science\", \"medical\"], \"primaryCategory\": \"health\", \"confidence\": 0.9}", "success": true, "metricsData": [{"name": "Answer Relevancy [GEval]", "threshold": 0.5, "success": true, "score": 1.0, "reason": "The output provides labels and a primary category that are directly relevant to the input text, which discusses a study on exercise reducing heart disease risk and improving mental health. The labels 'health', 'exercise', 'research', and 'disease prevention' accurately describe the main topics, and the high confidence score is appropriate. All evaluation steps are fully met.", "strictMode": false, "evaluationModel": "gpt-4.1", "evaluationCost": 0.0025859999999999998, "verboseLogs": "Criteria:\nEvaluate whether the actual output is topically relevant to the input text. The labels, categories, or analysis in the output should directly relate to the subject matter of the input. Structured metadata (labels, categories, confidence scores) that accurately describes the input text should be considered relevant. \n \nEvaluation Steps:\n[\n    \"Read the input text and identify its main topic or subject matter.\",\n    \"Examine the actual output to determine if its labels, categories, or analysis directly relate to the identified topic of the input text.\",\n    \"Check if any structured metadata in the output (such as labels, categories, or confidence scores) accurately describes the content of the input text.\",\n    \"Decide if the actual output maintains topical relevance to the input based on the above checks.\"\n] \n \nRubric:\nNone \n \nScore: 1.0"}], "runDuration": 4.128654750005808, "evaluationCost": 0.0025859999999999998, "order": 4}], "conversationalTestCases": [], "metricsScores": [{"metric": "JSON Schema Compliance [GEval]", "scores": [1.0, 1.0, 1.0, 1.0, 1.0], "passes": 5, "fails": 0, "errors": 0}, {"metric": "Classification Correctness [GEval]", "scores": [0.9, 0.9, 0.9, 0.8320821307318385, 0.9], "passes": 5, "fails": 0, "errors": 0}, {"metric": "Answer Relevancy [GEval]", "scores": [0.9029312229479407, 1.0, 1.0, 1.0, 1.0], "passes": 5, "fails": 0, "errors": 0}], "testPassed": 15, "testFailed": 0, "runDuration": 102.23429112503072, "evaluationCost": 0.04103999999999999}}